So the first part of Kinesis
that I like to discuss is Kinesis Data Streams.
Within the Kinesis family, there are several different
services that AWS offers for us to use
to handle our streaming data.
Now we'll go over each one of the services
over the next few lectures, but let's start
with Kinesis Data Streams.
Now the way that Kinesis Data Streams works
is by getting data from data producers.
And these data producers typically have
some type of JSON data as the payload.
Or they can use any type of data that
they can fit into a data blob.
And data producers are different devices
or applications that produce streaming data.
For instance, data producers can be things like
application log files, or social media streams.
They can be things like real time user interaction
from video games, they can be IoT devices
or manufacturing devices in a factory.
They can also be Clickstream data or user interaction
for an online website or application.
Data producers are the physical devices that produce
the data that we want to get into AWS.
Now once the data is produced, we can then use Kinesis
streams to transfer, or load, or stream that data into AWS.
And the way that Kinesis streams carries that data
is by using these things called shards.
Now, the way that I like to think about shards is a
container that holds the data that we want to send into AWS.
Now shards are made up of a couple of different parts
that we'll discuss over the next few slides.
Once the data is contained within a shard, then we use
data consumers to process that data or analyze that data.
Now data consumers can be things like EC2 Instances,
or it can be Lambda functions,
where we can do real time ETL processes.
We can also use another tool within the Kinesis family
called Kinesis Data Analytics to run real time
SQL queries on our streaming data.
Or we can send our streaming data to an EMR cluster,
and process it using Apache Spark.
And what's great about Kinesis Data Streams is we can have
multiple consumers consume the data stream,
or consume those shards, and perform different processes
depending on the job at hand.
Now, depending on what you wanna do
with your streaming data, you can use any of these consumers
to process the data, or further analyze the data,
or send it off to a data store within AWS,
like S3, DynamoDB, Redshift
or other business intelligence tools.
So just remember that when using Kinesis Data Streams,
if we wanted to store off our streaming data into let's say,
S3, we would have to use some intermediate service like
AWS Lambda to take the input
streaming data and output it onto S3.
Now in the next lecture, we'll talk about an service
that's made specifically for outputting streaming data
onto one of the AWS data stores.
But for now, let's go ahead and talk more in depth about
what shards are, because they're
an important part of Kinesis Data Streams.
So as mentioned earlier, shards are the rapper
or the container that contains all of the streaming data
that we want to load into AWS.
And a shard is made up of a couple of different parts.
Now there is a unique partition key for each shard.
So if you look at this example, we have two shards,
and the partition key for shard one is unique,
and then partition key for shard two is different
and it's unique as well.
Now each time that we make a request to send streaming data
through Kinesis streams, it creates a sequence
or as we depicted here, one of these little red boxes.
Each payload that we make creates a sequence
which is associated with a shard.
Now the more requests we make, and the larger
our payload gets, will determine how many shards we use.
Now the number of shards that you can use is from one to 500
by default, but you can request more shards if needed.
So another way that I like to break down what a shard
looks like or is made of, is by looking at this example.
Let's say we have a couple of different
railroad tracks lined up together.
Now the train ID is going to be the partition key.
So the first train up top is going to have
a unique partition key, and the train on bottom
is going to have a unique partition key as well.
Now each cart on the train
is considered the sequence number.
So if it has cart one, cart two, cart three,
that would be the sequence number associated with the shard.
And the actual passengers that are within
the train is considered the data.
So this is just a quick little example of
how I like to imagine shards and Kinesis Data Streams.
Now we've talked about what shards are made of
or what consists within a shard.
So let's take a look at what AWS offers for shards.
Now each shard consists of a sequence of data records,
you can think about those little red boxes.
These can be ingested at 1000 records per second.
Now, we already mentioned that the default limit of shards
is 500, but we can request increases to unlimited shards.
And we saw how those little red boxes or a train cart
is a data record, which is the unit
of data that's actually captured.
And this consists of the three parts, right?
A sequence number or the train cart, a partition key
or the ID for the train, and a data blob
or the people within the train.
And our data blob can be up to one megabyte in size.
Now an important feature about shards
is that they are a transient data store.
Meaning that the data is held within a shard
or the data records by default for 24 hours,
but we can increase that up to seven days.
So for example, if some type of failure happens
or your consumer application fails,
then the data within that stream is going
to be held on to by default for 24 hours.
But we can increase that up to seven days.
So it's essentially like a mini brain that remembers
the information or holds on to the information if needed.
So the next question is, well,
how do we actually load our data?
Or how do we actually get our data into a shard?
Or how do we get our data into Kinesis Data Streams?
Well, there's a few different ways to interact with
Kinesis Data Streams, and we'll talk about those now.
The first way that we can interact with Kinesis Data Streams
is by using a product called Kinesis Producer Library
or abbreviated as KPL.
And this is a easy to use library that allows you
to write to Kinesis Data Streams.
What it does is it allows you to use some
of the robust features and mechanisms of Kinesis
that you would normally have to write by hand,
if you were just using the Kinesis API.
Things like retry mechanisms, if for some reason
the stream didn't get processed.
Things like optimizing throughput and aggregating records
together, so you have the most optimal stream.
So what you can do is install the Kinesis Producer Library
on two EC2 Instances or integrate it directly
into your Java applications.
Now, once the data is in Kinesis Data Streams,
you can use Kinesis Client Library or abbreviated as KCL,
to directly interact with the Kinesis Producer Library
to consume and process data from the Kinesis Data Stream.
Both of these libraries are used to abstract some of
the low level commands that you would
have to use with the Kinesis API.
So the last way that I wanna talk about interacting with
Kinesis Data Streams is by using the Kinesis API.
Now with the Kinesis API, we can perform all of the same
actions that we can perform with the Kinesis Producer
Library or the Kinesis Client Library.
But it's used for more of low-level API operations
and more manual configurations.
So let's go ahead and take a look at some key features
between the Kinesis Producer Library and the Kinesis API.
With the Kinesis Producer Library, we said that it provides
a layer of abstraction specifically for ingesting data.
Now we can perform the same actions with the Kinesis API,
but things like automatic retry and the aggregation
of records for optimization is done automatically
with the Kinesis Producer Library.
And we would have to do it manually with the Kinesis API.
With the Kinesis API things like stream creation,
resharding, putting and getting records
are all manually handled.
Now one thing to remember about the Kinesis Producer Library
is that sometimes additional processing needs to be done.
So things like higher packaging efficiencies
and performance can be taken care of.
Now with the Kinesis API, there's no delay in processing.
But you may not be packaging and optimizing
the Kinesis Data Streams properly.
It really just depends on how you make the API calls.
So depending on your use case, if you need your data stream
immediately available within milliseconds,
it may be better to use the Kinesis API.
Because of the additional processing that could delay
your streaming data with the Kinesis Producer Library.
And finally, the Kinesis Producer Library
is used as a Java wrapper.
So you would have to implement it using Java.
While the Kinesis API you can use
any of the supported AWS SDKs.
Things like C sharp or JavaScript.
So just remember these features when deciding whether to use
the Kinesis Producer Library or the Kinesis API.
Now within the console,
it's pretty easy to create a Kinesis stream.
Essentially, you just provide a Kinesis stream name,
and then the number of shards that you want to use.
Now luckily, there's a little tool that you can use
within the console that allows you to estimate
the number of shards that you'll need,
depending on your payload size and the number of requests.
Now, you can always reshard, which enables you to increase
or decrease the number of shards in a stream.
And this is to adapt for any changes in the rate
of data flowing through the stream.
Now, resharding is typically done by a separate application
that monitors the metrics from the producers and consumers,
and makes an API call to add more shards if needed.
Now the documentation states that resharding is considered
an advanced option, but you do have the option to reshard,
or add, or decrease, the number of shards
after you create a stream by using the Kinesis API.
So let's answer the question.
Well, when should you use Kinesis Data Streams?
Well, when you need the data to be processed by consumers,
meaning that you're not looking to immediately
drop your data into a data store,
but rather, you would like some transformation done
or you'd like some analytics ran on it,
or you want to feed that data into another AWS service.
And just remember that with Kinesis Data Streams,
storing the data is optional.
You might just want to consume that data
and run some analytics on it, or feed it
into another service, but never store that data.
And finally, the last thing that's important with
Kinesis Data Streams is that it provides data retention.
This means that if the data is super important,
and you can't afford to lose the data,
data retention is built into Kinesis Data Streams
and allows you to hold on to that data
for 24 hours or up to seven days.
This is built in just in case there's some failure
within your application or your infrastructure,
and you need to reprocess the data that's within the stream.
And finally, let's take a look at some use cases
where we could use Kinesis Data Streams,
or where it would be a good option.
If we wanted to process and evaluate log files immediately,dynamo
we could use Kinesis Data Streams.
So for example, if we had an application running
and we needed to process and evaluate the log files
that that application produces, we're able to analyze
and continuously process that data within seconds.
We could run analytics on the log files,
we could do some type of transformation.
And we could also store that data off
into another data source like S3.
Another example is if we wanted to run
real time Clickstream data analytics.
So for example, if we had an e-commerce site,
and we wanted to know what a user was clicking on,
and what products they liked, we could use
Kinesis Data Streams to stream that information
and analyze that data, and offer them real time suggestions
on other products that they might like.
So I hope this gives you a good understanding
of what Kinesis Data Streams is, what shards are,
and how we can use Kinesis Data Streams
throughout our machine learning process.
So that's it for this lecture.
If you have any questions then please post
in the course form.
If not, then let's go ahead and move on to the next lecture.