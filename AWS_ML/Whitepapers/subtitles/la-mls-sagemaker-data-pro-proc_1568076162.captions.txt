In this lesson, we're going to take a look at some of the data preprocessing steps that we can take that we may want to take that we may want to do to our data before we start to use it in machine learning and we'll have a bit of a word about what we can use inside of SageMaker to help us do that. Now, I suppose one thing I want to say before we get into this and it's a common theme throughout the whole of data science and the whole of machine learning and this course, but not something I've necessarily spoken very much about and that's the big sort of elephant in the room. If you like that sort of there before we do any machine learning task and that's something that's probably quite obvious on when I say it, you're going to say, well, that's kind of obvious, but it's got to be said, and that's the business problem or the problem that we're actually trying to solve. We need to have a good understanding of the problem we're trying to solve before we try any machine learning solution, we need to have a goal in mind and then, with that goal in mind, we're able to collect the right data, process it in the right way and select the right machine learning algorithm to be able to create our model from and without that foresight, without their understanding of the business problem or the problem, we're not going to get very far, so there is no machine learning process if you like where you can just gather a bunch of data, throw at up the machine learning algorithm and say, tell me something. Even with the unsupervised algorithms, you still have to have something in mind that you want to understand, even though you don't have labeled data in the first place and so this actually dovetails into data preprocessing because it's in this stage that we're taking a look at the data more closely and we'll be able to see if it aligns to the business case, the business problem or the social problem that we're trying to solve. So let's have a look at data preprocessing on the different steps or stages as I've defined them here, and it's important to sort of realize that we are just talking about processing data. Munging date around doing stuff with data and, of course, it's almost limitless. What you can do with data. It's really just limited on the amount of time that you're willing to spend on the exercise. So let's take a look at some of the data preprocessing steps, which you can take and the first one here, is to visualize your data. So the data has been sourced from a scientific study or from a massive IOT sensors or from a scrape of the internet. So many machine learning projects seem to start with I've downloaded the entire contents of Wikipedia or IMDB and now I'm going to apply it to this. So once you've done that, we might want to be able to visualize the data. Now, this may be the first thing you do. It depends whether the data is sufficiently formatted to be able to visualize it. Now we've talked about using Amazon quicksight to do some of that visualization, and you absolutely can inside of this SageMaker realm we do have Jupyter notebooks, which really don't do form almost the core central service, which provides a data preprocessing platform. So we've also seen how with Jupyter notebooks you're able to load data in and use things like matplotlib to be able to draw graphs to be able to visualize image data to be able to see the data you've got. Now, in addition to just viewing the raw data itself, you might want to start to put your data into a machine learning algorithm to actually start to visualize the data. So we've looked at this in relation to things like K-means clustering. We might want to put our data into that kind of an algorithm so that we can get a visualization out of some clusters of data that might then provide us with insight for a way that maybe we want to label our data with that cluster information and then run that data then through a different algorithm, so visualizing the data can be for lots of different reasons just a sanity check that the data looks like what you expected it to look like to give you a sense that you need to do more processing on that data and in SageMaker, the place to visualize data is inside of our Jupyter notebooks and this sort of goes hand in hand with exploring our data so we can use again our Jupyter notebook environments with tabular data with image data with different libraries that we can get hold of from the vast amount of libraries in the Python community to be able to say, okay, well, let's run some analysis over this data. Let's do some statistical analysis, not machine learning now, but just some statistical analysis over our data so that we get a sense of what it is that we've got and of course, that's a no entire topic on almost an entire course in its own right. But let's explore the data, see what we've got and figure out whether we need to manipulate the data in any particular way and if we do, then that's where we open up the possibilities of engineering. So let's do some feature engineering in our data. Now we've looked at this before and a practical way we can do it again is to yeah, if the data is small enough inside of a Jupyter notebook, we can do feature engineering just in there, and we can process through hundreds of thousands of records in a table if we wanted to inside of a Jupyter notebook environment and if the data is vast, if that ater is large and we've got it inside of a data warehouse, then we might decide that we want to use EMR and EMR with Spark to do some processing of that data in that environment so that we can engineer some features from our data set. So with the exploration and the visualization of the data, we could look at the data and see that there are some relevant features here but those features on their own are a bit of a almost like a waste of space to have each one of them individually, and we want to combine them together and do some feature engineering. We want to do some one hot encoding. Want to make sure that our data is represented in a way that we know that machine learning algorithms will be able to make the best use of that data. Then we've got synthesize. We might have to synthesize some data. Now we've looked at scenarios where we have a skewed data set. So, for example, when we're looking at a manufacturing data set and we're looking for errors or we're looking for faults in devices or units which have which have bean manufactured in a manufacturing plant a physical device? Well, hopefully in that in that plant, the amount of failures they have is relatively small. That's what they would be hoping for anyway and we're looking for the characteristics of when a failure has occurred and we wanting to use machine learning to do that. Well, the problem is that our data set is heavily skewed towards data that doesn't have the problem. Now, there are some machine learning algorithms which will be able to be better at that with that sku of data but ultimately, we need to get more data. We we want to get more data so that we can train a model on what to look for. Now, the primary thing is we've talked about before the primary thing there is to go back and say, have you got more data? I am the data engineering thing, the machine learning specialist. I'm going to put this model together for you. But if you have more examples of what I'm looking for, then I absolutely want that. Now, if you don't have that, then we have to sort of put our thinking caps on and look at the data. We do have and see if we can synthesize some data from what we have. So change some of the data subtly with a little bit of random variance to synthesize what acceptable parameters might look like for a failure rate. So we might want to synthesize that data there and also, when you're looking at image data so when you have captured a whole bunch of images so actual pictorial images of something that you're looking to recognize from, say, a convolution all new your network, you're wanting to use it to be able to recognize an image on classify an image will often is the case that we're limited on the number of images that we can get hold of so we can synthesize new images from the images we have and that is as simple as potentially just rotating the image, changing the contrast or the color ratios inside of that image just a little bit, because photographs and images from the real world from different cameras from different devices. Taking those images, I got a very subtly and maybe we're going to be looking at the object from different angles. So if we have a nice, neat data set with just a small number of images nicely and neatly taken all from a particular orientation, well, we can get a probably a better quality of training by adding some synthesized data. That's to rotate the image, to adjust the contrast of the image, to adjust the color and things like that and creating a much larger image set in the process. So once we got there, we visualized our data have explored it. We've done any engineering and any synthesis that we want to do. We then have to look at more of the practical side of getting this data into the machine learning algorithm and we at this point will probably have a pretty good idea about which algorithm it is that we want to target and so when we look at the documentation for their algorithm, if it's not one that we've made ourselves, it's going to tell us that way that it wants to have that data presented and so we might need to start at looking some conversion. So to convert the data into whatever that algorithm is looking for now, these kinds of conversions could be anything from taking a CSV file and converting it into a null py array or it could be taking a set of images and creating a record IO that can be piped into the algorithm to learn and it can also change the structure of the data as well. So it could be that what the images example is a pretty good one. We could have the whole directory full of images. That's fine. But if we want to be able to pipe it into an algorithm, we might want to take all of those images and combine them together into a single structure. Another example of this is that we might have a table of data on that table of data might be labeled, but maybe the label is in a separate file. Now some algorithms might be okay with that, but others aren't so we might need to combine in one single structure our table with our label and put it all together into a single structure So it's really just data manipulation and a lot of this can come down to just scripting and just bashing away on the data until you get into a structure that you needed to be and then finally, maybe some algorithms will do this for you. But you may need to split the data. So split the data that we have all of the data coming in. And so okay, well, this is going to be my training data and then this is going to be my testing data or my validation data or my training test and validation data again, depending on the process that you're going to use in the next step when we come to train our model. So these are the steps. These are the components that I am highlighting in data preprocessing. Obviously, some of these sort of work together and some of these kind of overlap. But generally speaking, we can visualize we can explore our data. We might need to do some feature engineering. We night might need to synthesize some data so that we can expand out the data set we have for better quality machine learning. Then there's the transformation of our data to convert it into the right kind of file format. The right kind of file structure and then split our data out into the different sets that we need to support the training that we're about to do and inside of Amazon SageMaker. The components that are going to you support this probably primarily are SageMaker notebooks, SageMaker notebooks being that development environment that you can figure out you can explore, you can visualize the data. You can also use it, of course, to be able to import data from other places like EMR. That you can use it to export data into other services put it into S3 going visualize it in quicksite. You can use notebooks as your little central area for being able to do all of these operations and then we also need to remember the SageMaker algorithms as well. Now I'm not getting ahead of myself. I'm not saying Okay, we need to start training now in the data preprocessing stage. What I'm saying is that we can still use algorithms to help us to be able to explore the data so we can use principle components analysis. If we need to reduce the dimensionality of the data, or if we need to be able to visualize the data by reducing it down to two dimensions or three dimensions, for example, and we could use K means algorithm so unsupervised training algorithms to help us to be able to explore the data more. So remember, even at this stage, we can still be using some of those machine learning algorithms as a preprocessing stage for our training.