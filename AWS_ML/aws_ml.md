[TOC]





# Exam Logistics

- 170 min
- 65 questions
- Contents:
  - Data engineering - 20%
  - Exploratory data analysis - 24%
  - Modellling - 36%
  - ML implementation & operation  20%
- Type:
  - **Multiple choice**
  - **Multiple response**

å»ºè®®ï¼š

- å…ˆè¯»é¢˜ï¼Œå°è¯•åœ¨çœ‹é€‰é¡¹å‰ç­”é¢˜ã€‚

- æ‰¾å…³é”®è¯ï¼ˆqualifier & key phraseï¼‰ï¼Œå¹¶æ ¹æ®æ­¤å»æ‰é”™è¯¯é€‰é¡¹ã€‚

- å®åœ¨ä¸ä¼šï¼Œå…ˆè·³è¿‡ã€‚

[exam preparation path](https://aws.amazon.com/training/learning-paths/machine-learning/exam-preparation/)

  


## Part I: Data engineering - 20%
### 1ï¼ŒCreate data repositories for ML

æ•°æ®å½¢å¼[structured, unstruced] -> a centralized repository -> Data Lake

<img src="awsml_pic/data_source.png" alt="data_source" style="zoom:50%;" />

AWS Lake Formation 

Amason S3 storage option for ds processing on AWS


### 2ï¼ŒIdentify and implement a data-ingestion solution
### 3ï¼ŒIdentify and implement a data-transformation solution

  

## Part II: Exploratory data analysis - 24%















## Part III: Modellling - 36%

## Part IV: ML implementation & operation  20%





# ML for Business Leaders

## When & How?

### 1,when?

when is machine learning a proporate tool to solve my problem?

what can? what tools?

<img src="awsml_pic/what_ml_can.png" alt="what_ml_can" style="zoom:50%;" />

When not to use?

no data

no groundtruth labels

quick launch

no tolerance for error

### 2, Six questions to ask?

1, what are the made assumption?

2, what is the learning target / hypothesis? (hypothesis testing for large datasets is basic promise for ML)

3, what type of ML problem is it?

4, why did you choose this algorithm? (simpler baseline?)

5, how will you evaluate the model performance?

6, how confident are you that u can generalize the results?		

### 3, How?

how to identify ML opportunities?

**Amazon ML applications:**

recommendations

robotics optimizations

forecasting

search optimizations

delivery routes

Alexa

### 4, Define and scope a ML problem?

ML is the subfield of AI, prevalence of large data sets and massive computational resources has made the domaniance the field of AI. 

![ml_map](awsml_pic/ml_map.png)

| i,define problem                                             | ii, input gathering                                          | iii, output                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| <img src="awsml_pic/define_probelm.png" alt="define_probelm" style="zoom:50%;" /> | <img src="awsml_pic/input.png" alt="input" style="zoom:50%;" /> | <img src="awsml_pic/output.png" alt="output" style="zoom:50%;" /> |

##  ML Process

![MLprocess](awsml_pic/MLprocess.png)

| Feature Engineering domain specific | <img src="awsml_pic/domain_specific.png" alt="domain_specific" style="zoom:50%;" /> |
| ----------------------------------- | ------------------------------------------------------------ |
| **Parameter Tuning**                | - loss function [å’Œground truthçš„å·®åˆ«]<br/>- regularisation [increase the generalization to better fit the data]<br/>- learning parameters (decay rate æ§åˆ¶modelå­¦ä¹ çš„å¿«æ…¢)<img src="awsml_pic/parameter_tunning.png" alt="parameter_tunning" style="zoom:50%;" /> |


## Evaluation 

#### 1, Overfitting vs underfittingï¼ˆgeneralize more toward unseen dataï¼‰

- use validation error 

- using training error -> overfitting, lack of feature/information -> undercutting

#### 2, Bias-variance tradeoff [supervised]

<img src="awsml_pic/bias-variance_tradeoff.png" alt="bias-variance_tradeoff" style="zoom:50%;" />

#### 3, evaluation matrix

| æ¨¡å‹       | evaluation                                      |                      |
| ---------- | ----------------------------------------------- | -------------------- |
| Regression | <img src="awsml_pic/regression_eva.png"  /> | -RMSEï¼ŒMAPE è¶Šå¤§è¶Šå¥½<br> -R^2 è¶Šå¤§è¶Šå¥½ |
| Classification | - confusion matrix<img src="awsml_pic/confusion_matrix.png" alt="confusion_matrix" style="zoom:50%;" /> <br/> -precision recall <img src="awsml_pic/presion_recall.png" alt="presion_recall" style="zoom:50%;" /> | - precision: how correct we are on ones we predictect would be positive <br/> - recall: fraction of negatives that we wrongly predicted<br>i.e. search engine; precision, quality and how relevant it is; completeness and fraction of relevance |
| Binary classification ä¾‹å­ | - type I: alpha ~ 5%<br/>- type II: beta 1- power<br/>- power ~ 80% [ä¾æƒ…å†µè®¢]<br/><img src="awsml_pic/type12error.png" alt="type12error" style="zoom:50%;" /><img src="awsml_pic/binary.png" alt="binary"  /> <br> - specificity = TN/TN + FP <br> - FPR = 1- specificity = FP/ TN + FP<br> \ | - precisionï¼šåœ¨æˆ‘ä»¬åˆ¤æ–­æ˜¯diseaseä¸­æœ‰å¤šå°‘äººæ˜¯çœŸçš„ç—…äº†<br>- recall: åœ¨æœ‰diseaseçš„æ ·æœ¬é‡ä¸­ï¼Œæœ‰å¤šå°‘æˆ‘ä»¬å¯ä»¥æ­£ç¡®çš„åˆ¤æ–­å‡ºæ¥ã€‚<br>- accuracy: æ­£ç¡®åˆ¤æ–­çš„[overall]ã€‚<br> |
| ROC <br>AUC | <img src="awsml_pic/roc.png" alt="a" style="zoom:30%;" /><img src="awsml_pic/a.png" alt="a" style="zoom:30%;" /> | ROC: <br/>1, é€‰æ‹©ä¸åŒçš„thresholdï¼ŒTPR å’Œ FPR å¯¹åº”å…³ç³»ã€‚<br/>2, FPRè¶Šå°ï¼ŒTPRè¶Šå¤§ã€‚å…¨å±€æœ€ä¼˜è§£ï¼Œèƒ½æ¥å—çš„FPRå·¦è¾¹èƒ½æ¥å—çš„ç‚¹ã€‚<br/> AUC: auc é¢ç§¯è¶Šå¤§ï¼Œæ¨¡å‹è¶Šå¥½ |

### Key issues in ML

[AWS: The elements of Data Science](https://aws.amazon.com/training/learning-paths/machine-learning/exam-preparation/)

#### Data quality

consistency of the data (å’Œé—®é¢˜ä¸€è‡´ä¹ˆ)

accuracy of the data

noisy data (fluctuation in the input and output)

missing data (é‚£äº›æ¨¡å‹å¯¹missingæ•æ„Ÿï¼Ÿ)

outliers 

bias

Variance

#### Model quality

**overfitting and underfitting**

![overandunder](awsml_pic/overandunder.png)

#### Computation speed and scalability

use sagemaker and EC2 

- increase speed

- solve prediction time complexity

- solve space complexity



# The elements of ML

## Supervised learning:

### Linear methods:

| Linear                                   | ![linear](awsml_pic/linear.png)                             |
| ---------------------------------------- | ----------------------------------------------------------- |
| **Linear regression (univariate)**       | <img src="awsml_pic/lr.png" alt="lr" style="zoom:85%;" />   |
| **Multivariate LR**<br>Multicollinearity | <img src="awsml_pic/mlr.png" alt="mlr" style="zoom:55%;" /> |

### Logistic regression

|                               | <img src="awsml_pic/logistic.png" alt="logistic" style="zoom:67%;" /> |
| ----------------------------- | ------------------------------------------------------------ |
| sigmoid curve                 | is a good representation of probability which is widely used in logistic regression to fit a model. x [-inf, inf] --> y [0 or 1] <br><img src="awsml_pic/sigmoid.png" alt="sigmoid" style="zoom:50%;" /> |
| Logit function                | ![lgr](awsml_pic/lgr.png) <br>The **logit** function is the inverse of the logistic function. <br/>![l](awsml_pic/l.png) |
| fit logistic regression model | <img src="awsml_pic/fitsigmoid.png" alt="fitsigmoid" style="zoom:60%;" /><img src="awsml_pic/logit.png" alt="logit" style="zoom:67%;" /> |





problem formulation

## data collection - EDA 

### sampling 

[representivity of expected production population: unbiased]

- Random sampling
  - é—®é¢˜1: rare subpopulation can be underrepresented
  - è§£å†³1ï¼š**Stratified sampling**
    - random sampling to each subpopulation.
      - if sampling probability is not the same for each stratum, weights can be used in metrics. 
  - é—®é¢˜2ï¼š
    - sensonality
      - è§£å†³ï¼šåˆ†å±‚æŠ½æ ·å¯ä»¥å‡å°‘bias | å¯è§†åŒ–
    - trends
      - è§£å†³ï¼šæ¯”è¾ƒä¸åŒæ—¶é—´æ®µçš„æ¨¡å‹ç»“æœ | å¯è§†åŒ–
  - é—®é¢˜3ï¼š
    - leakage
      - Train/test bleed: training test data é‡å¤
      - åœ¨trainä¸­ç”¨äº†ä½†æ˜¯productionä¸ç”¨

### labeling

**Amazon Mechanical Turk** (human intelligence tasks, äººå·¥æ ‡è®°é—®å·è°ƒæŸ¥)

- plurality (assign same HIT to multiple labellers)
- gold standard hits (known labels mixed æµ‹è¯•æ ‡è®°è¡¨ç°)
- auditors

![causal_corr](awsml_pic/causal_corr.png)

### Data Schema

pandas merge/join

### Data Statistics

### ğŸŒŸdescriptive statistics

![descrip](awsml_pic/descrip.png)

```python
pd.describe()
pd.hist()
sns.distplot()  #æœ‰histogram + ked ï¼ˆsmoothingæ‹Ÿåˆå†…æ ¸å¯†åº¦ä¼°è®¡ï¼‰
df['x'].value_counts()
```
##### basic plots

![plot1](awsml_pic/plot1.png)

##### sns.distplot()

æ ¸å¯†åº¦ä¼°è®¡Kernel Density Estimation(KDE)æ˜¯åœ¨æ¦‚ç‡è®ºä¸­ç”¨æ¥ä¼°è®¡æœªçŸ¥çš„å¯†åº¦å‡½æ•°ï¼Œå±äºéå‚æ•°æ£€éªŒæ–¹æ³•ä¹‹ä¸€ã€‚

ç›´æ–¹å›¾ï¼šå¯†åº¦å‡½æ•°æ˜¯ä¸å¹³æ»‘çš„ï¼›å¯†åº¦å‡½æ•°å—å­åŒºé—´ï¼ˆå³æ¯ä¸ªç›´æ–¹ä½“ï¼‰å®½åº¦å½±å“å¾ˆå¤§ï¼ŒåŒæ ·çš„åŸå§‹æ•°æ®å¦‚æœå–ä¸åŒçš„å­åŒºé—´èŒƒå›´ï¼Œé‚£ä¹ˆå±•ç¤ºçš„ç»“æœå¯èƒ½æ˜¯å®Œå…¨ä¸åŒçš„ã€‚

[æ ¸å¯†åº¦ä¼°è®¡kde](https://www.jianshu.com/p/428ae3658f85)

![kde](awsml_pic/kde.png)

#### ğŸŒŸcorrelation

##### scatter | scatter_matrix

scatterplot matrix (linear relationship) - visualize attribute-target and attribute-attribute pairwise relationships.



| Scatter <br>Scatter_matrix      | ![scatter](awsml_pic/scatter.png) |
| ------------------------------- | --------------------------------- |
| **scatter for binary classes**  | ![scatter](awsml_pic/scatter1.png) |
| **Correlation matrix heat map** |  ![scatter](awsml_pic/heatmap.png) <br>![scatter](awsml_pic/heatmap_sns.png)|
| **Pearson correlation** | ![scatter](awsml_pic/pearson.png) |



### Data issues

| X        | ![scatter](awsml_pic/di.png)  |
| -------- | ----------------------------- |
| **x->y** | ![scatter](awsml_pic/di2.png) |



## Data Processing and Feature Engineering



### **Data Preprocessing:  Encoding Categorical Variables**

### **Data Preprocessing: Encoding Nominals**

### **Data Preprocessing: Handling Missing Values**

### **Feature Engineering**

### **Feature Engineering: Filtering and Scaling**

### **Feature Engineering: Transformation**

### **Feature Engineering: Text-Based Features**





## Model Training, Tuning, and Debugging

### **Supervised Learning: Neural Networks**

### **Supervised Learning: K-Nearest Neighbors**

### **Supervised Learning: Linear and Non-Linear Support Vector Machines**

### **Supervised Learning: Decision Trees and Random Forests**

### Model Training: Validation Set

### **Model Training: Bias Variance Tradeoff**

### **Model Debugging: Error Analysis**

### **Model Tuning: Regularization**

### **Model Tuning: Hyperparameter Tuning**

### **Model Tuning**

### **Model Tuning: Feature Extraction**

### **Model Tuning: Feature Selection**

### **Model Tuning: Bagging/Boosting**

