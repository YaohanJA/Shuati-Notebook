[TOC]

# Exam Logistics

- 170 min
- 65 questions
- Contents:
  - Data engineering - 20%
  - Exploratory data analysis - 24%
  - Modellling - 36%
  - ML implementation & operation  20%
- Type:
  - **Multiple choice**
  - **Multiple response**

å»ºè®®ï¼š

- å…ˆè¯»é¢˜ï¼Œå°è¯•åœ¨çœ‹é€‰é¡¹å‰ç­”é¢˜ã€‚

- æ‰¾å…³é”®è¯ï¼ˆqualifier & key phraseï¼‰ï¼Œå¹¶æ ¹æ®æ­¤å»æ‰é”™è¯¯é€‰é¡¹ã€‚

- å®åœ¨ä¸ä¼šï¼Œå…ˆè·³è¿‡ã€‚

[exam preparation path](https://aws.amazon.com/training/learning-paths/machine-learning/exam-preparation/)

  


## Part I: Data engineering - 20%
### 1ï¼ŒCreate data repositories for ML

æ•°æ®å½¢å¼[structured, unstruced] -> a centralized repository -> Data Lake

<img src="awsml_pic/data_source.png" alt="data_source" style="zoom:50%;" />

AWS Lake Formation 

Amason S3 storage option for ds processing on AWS


### 2ï¼ŒIdentify and implement a data-ingestion solution
### 3ï¼ŒIdentify and implement a data-transformation solution

  

## Part II: Exploratory data analysis - 24%















## Part III: Modellling - 36%

## Part IV: ML implementation & operation  20%





# ML for Business Leaders

## When & How?

### 1,when?

when is machine learning a proporate tool to solve my problem?

what can? what tools?

<img src="awsml_pic/what_ml_can.png" alt="what_ml_can" style="zoom:50%;" />

When not to use?

no data

no groundtruth labels

quick launch

no tolerance for error

### 2, Six questions to ask?

1, what are the made assumption?

2, what is the learning target / hypothesis? (hypothesis testing for large datasets is basic promise for ML)

3, what type of ML problem is it?

4, why did you choose this algorithm? (simpler baseline?)

5, how will you evaluate the model performance?

6, how confident are you that u can generalize the results?		

### 3, How?

how to identify ML opportunities?

**Amazon ML applications:**

recommendations

robotics optimizations

forecasting

search optimizations

delivery routes

Alexa

### 4, Define and scope a ML problem?

ML is the subfield of AI, prevalence of large data sets and massive computational resources has made the domaniance the field of AI. 

![ml_map](awsml_pic/ml_map.png)

| i,define problem                                             | ii, input gathering                                          | iii, output                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| <img src="awsml_pic/define_probelm.png" alt="define_probelm" style="zoom:50%;" /> | <img src="awsml_pic/input.png" alt="input" style="zoom:50%;" /> | <img src="awsml_pic/output.png" alt="output" style="zoom:50%;" /> |

##  ML Process

![MLprocess](awsml_pic/MLprocess.png)

| Feature Engineering domain specific | <img src="awsml_pic/domain_specific.png" alt="domain_specific" style="zoom:50%;" /> |
| ----------------------------------- | ------------------------------------------------------------ |
| **Parameter Tuning**                | - loss function [å’Œground truthçš„å·®åˆ«]<br/>- regularisation [increase the generalization to better fit the data]<br/>- learning parameters (decay rate æ§åˆ¶modelå­¦ä¹ çš„å¿«æ…¢)<img src="awsml_pic/parameter_tunning.png" alt="parameter_tunning" style="zoom:50%;" /> |


## Evaluation 

#### 1, Overfitting vs underfittingï¼ˆgeneralize more toward unseen dataï¼‰

- use validation error 

- using training error -> overfitting, lack of feature/information -> undercutting

#### 2, Bias-variance tradeoff [supervised]

<img src="awsml_pic/bias-variance_tradeoff.png" alt="bias-variance_tradeoff" style="zoom:50%;" />

#### 3, evaluation matrix

| æ¨¡å‹       | evaluation                                      |                      |
| ---------- | ----------------------------------------------- | -------------------- |
| Regression | <img src="awsml_pic/regression_eva.png"  /> | -RMSEï¼ŒMAPE è¶Šå¤§è¶Šå¥½<br> -R^2 è¶Šå¤§è¶Šå¥½ |
| Classification | - confusion matrix<img src="awsml_pic/confusion_matrix.png" alt="confusion_matrix" style="zoom:50%;" /> <br/> -precision recall <img src="awsml_pic/presion_recall.png" alt="presion_recall" style="zoom:50%;" /> | - precision: how correct we are on ones we predictect would be positive <br/> - recall: fraction of negatives that we wrongly predicted<br>i.e. search engine; precision, quality and how relevant it is; completeness and fraction of relevance |
| Binary classification ä¾‹å­ | - type I: alpha ~ 5%<br/>- type II: beta 1- power<br/>- power ~ 80% [ä¾æƒ…å†µè®¢]<br/><img src="awsml_pic/type12error.png" alt="type12error" style="zoom:50%;" /><img src="awsml_pic/binary.png" alt="binary"  /> <br> - specificity = TN/TN + FP <br> - FPR = 1- specificity = FP/ TN + FP<br> \ | - precisionï¼šåœ¨æˆ‘ä»¬åˆ¤æ–­æ˜¯diseaseä¸­æœ‰å¤šå°‘äººæ˜¯çœŸçš„ç—…äº†<br>- recall: åœ¨æœ‰diseaseçš„æ ·æœ¬é‡ä¸­ï¼Œæœ‰å¤šå°‘æˆ‘ä»¬å¯ä»¥æ­£ç¡®çš„åˆ¤æ–­å‡ºæ¥ã€‚<br>- accuracy: æ­£ç¡®åˆ¤æ–­çš„[overall]ã€‚<br> |
| ROC <br>AUC | <img src="awsml_pic/roc.png" alt="a" style="zoom:30%;" /><img src="awsml_pic/a.png" alt="a" style="zoom:30%;" /> | ROC: <br/>1, é€‰æ‹©ä¸åŒçš„thresholdï¼ŒTPR å’Œ FPR å¯¹åº”å…³ç³»ã€‚<br/>2, FPRè¶Šå°ï¼ŒTPRè¶Šå¤§ã€‚å…¨å±€æœ€ä¼˜è§£ï¼Œèƒ½æ¥å—çš„FPRå·¦è¾¹èƒ½æ¥å—çš„ç‚¹ã€‚<br/> AUC: auc é¢ç§¯è¶Šå¤§ï¼Œæ¨¡å‹è¶Šå¥½ |

### Key issues in ML

[AWS: The elements of Data Science](https://aws.amazon.com/training/learning-paths/machine-learning/exam-preparation/)

#### Data quality

consistency of the data (å’Œé—®é¢˜ä¸€è‡´ä¹ˆ)

accuracy of the data

noisy data (fluctuation in the input and output)

missing data (é‚£äº›æ¨¡å‹å¯¹missingæ•æ„Ÿï¼Ÿ)

outliers 

bias

Variance

#### Model quality

**overfitting and underfitting**

![overandunder](awsml_pic/overandunder.png)

#### Computation speed and scalability

use sagemaker and EC2 

- increase speed

- solve prediction time complexity

- solve space complexity



# The elements of ML


## data collection 

### sampling 

[representivity of expected production population: unbiased]

- Random sampling
  - é—®é¢˜1: rare subpopulation can be underrepresented
  - è§£å†³1ï¼š**Stratified sampling**
    - random sampling to each subpopulation.
      - if sampling probability is not the same for each stratum, weights can be used in metrics. 
  - é—®é¢˜2ï¼š
    - sensonality
      - è§£å†³ï¼šåˆ†å±‚æŠ½æ ·å¯ä»¥å‡å°‘bias | å¯è§†åŒ–
    - trends
      - è§£å†³ï¼šæ¯”è¾ƒä¸åŒæ—¶é—´æ®µçš„æ¨¡å‹ç»“æœ | å¯è§†åŒ–
  - é—®é¢˜3ï¼š
    - leakage
      - Train/test bleed: training test data é‡å¤
      - åœ¨trainä¸­ç”¨äº†ä½†æ˜¯productionä¸ç”¨

### labeling

**Amazon Mechanical Turk** (human intelligence tasks, äººå·¥æ ‡è®°é—®å·è°ƒæŸ¥)

- plurality (assign same HIT to multiple labellers)
- gold standard hits (known labels mixed æµ‹è¯•æ ‡è®°è¡¨ç°)
- auditors

![causal_corr](awsml_pic/causal_corr.png)

## EDA

### Data Schema

pandas merge/join

### Data Statistics

### ğŸŒŸdescriptive statistics

![descrip](awsml_pic/descrip.png)

```python
pd.describe()
pd.hist()
sns.distplot()  #æœ‰histogram + ked ï¼ˆsmoothingæ‹Ÿåˆå†…æ ¸å¯†åº¦ä¼°è®¡ï¼‰
df['x'].value_counts()
```
#### basic plots

![plot1](awsml_pic/plot1.png)

#### sns.distplot()

æ ¸å¯†åº¦ä¼°è®¡Kernel Density Estimation(KDE)æ˜¯åœ¨æ¦‚ç‡è®ºä¸­ç”¨æ¥ä¼°è®¡æœªçŸ¥çš„å¯†åº¦å‡½æ•°ï¼Œå±äºéå‚æ•°æ£€éªŒæ–¹æ³•ä¹‹ä¸€ã€‚

ç›´æ–¹å›¾ï¼šå¯†åº¦å‡½æ•°æ˜¯ä¸å¹³æ»‘çš„ï¼›å¯†åº¦å‡½æ•°å—å­åŒºé—´ï¼ˆå³æ¯ä¸ªç›´æ–¹ä½“ï¼‰å®½åº¦å½±å“å¾ˆå¤§ï¼ŒåŒæ ·çš„åŸå§‹æ•°æ®å¦‚æœå–ä¸åŒçš„å­åŒºé—´èŒƒå›´ï¼Œé‚£ä¹ˆå±•ç¤ºçš„ç»“æœå¯èƒ½æ˜¯å®Œå…¨ä¸åŒçš„ã€‚

[æ ¸å¯†åº¦ä¼°è®¡kde](https://www.jianshu.com/p/428ae3658f85)

![kde](awsml_pic/kde.png)

### ğŸŒŸcorrelation

#### scatter | scatter_matrix

scatterplot matrix (linear relationship) - visualize attribute-target and attribute-attribute pairwise relationships.



| Scatter <br>Scatter_matrix      | ![scatter](awsml_pic/scatter.png) |
| ------------------------------- | --------------------------------- |
| **scatter for binary classes**  | ![scatter](awsml_pic/scatter1.png) |
| **Correlation matrix heat map** |  ![scatter](awsml_pic/heatmap.png) <br>![scatter](awsml_pic/heatmap_sns.png)|
| **Pearson correlation** | ![scatter](awsml_pic/pearson.png) |



### Data issues

| X        | ![scatter](awsml_pic/di.png)  |
| -------- | ----------------------------- |
| **x->y** | ![scatter](awsml_pic/di2.png) |



## Data Processing and Feature Engineering 



### **Data Preprocessing:  Encoding Categorical Variables**

### **Data Preprocessing: Encoding Nominals**

### **Data Preprocessing: Handling Missing Values**

### **Feature Engineering**

### **Feature Engineering: Filtering and Scaling**

### **Feature Engineering: Transformation**

### **Feature Engineering: Text-Based Features**





## Model Training, Tuning, and Debugging

## Supervised learning:

### Linear methods

| Linear                                | ![linear](awsml_pic/linear.png?lastModify=1587378794) |
| ------------------------------------- | ------------------------------------------------------------ |
| **Linear regression (univariate)**    | ![lr](awsml_pic/lr.png?lastModify=1587378794) |
| **Multivariate LR** Multicollinearity | ![mlr](awsml_pic/mlr.png?lastModify=1587378794) |

### Logistic regression

|                               | ![logistic](awsml_pic/logistic.png?lastModify=1587378794) |
| ----------------------------- | ------------------------------------------------------------ |
| sigmoid curve                 | is a good representation of probability which is widely used in logistic regression to fit a model. x [-inf, inf] --> y [0 or 1]  ![sigmoid](awsml_pic/sigmoid.png?lastModify=1587378794) |
| Logit function                | ![lgr](awsml_pic/lgr.png?lastModify=1587378794)  The **logit** function is the inverse of the logistic function.  ![l](awsml_pic/l.png?lastModify=1587378794) |
| fit logistic regression model | ![fitsigmoid](awsml_pic/fitsigmoid.png?lastModify=1587378794)![logit](awsml_pic/logit.png?lastModify=1587378794) |



### Neural Networks

#### Perceptron

 [input: linear, 1 layer]

![perceptron](awsml_pic/perceptron.png)

#### Neural networks

Scikit-learn: sklearn.neural_network.MLPClassifier
![neural_network](awsml_pic/neural_network.png)

| ç‰¹ç‚¹                                                     | Deep learning frameworks                   |
| -------------------------------------------------------- | ------------------------------------------ |
| Hard to interpret<br>expensive to train, fast to predict | PyTorch<br/>Caffe<br/>TensorFlow<br/>MXnet |

##### CNN

convolutional neural networks - classify images

å·ç§¯ç¥ç»ç½‘ç»œ

power image search services, self-driving cars, automatic video classification systems
voice recognition
natural language processing

![neural_network](awsml_pic/cnn.png)

å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰

[CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)

[Introducing convolutional networks](http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks)

Convolutional Layer: 

use kernel as features to extract local features

input image, filters to convolve with the image to create the next layer.



Pooling layer: (dimension reduction)

> Aggregate local information
>
> Produces a smaller imageâ€¨
>  (each resulting pixel captures some â€œglobalâ€ information)
>
> If object in input image shifts a little, output is the same

max pooling 

avg pooling



convert tensor to vector 

Category

|                               | è¾“å…¥                                     |                                                     | è¾“å‡º                                                        |
| :---------------------------- | ---------------------------------------- | --------------------------------------------------- | ----------------------------------------------------------- |
| å·ç§¯å±‚<br>Convolutional Layer | local receptive fields <br>28 x 28 pixel | filter \|kernelè¿‡æ»¤å™¨ <br>receptive fields<br>5 x 5 | æ¿€æ´»æ˜ å°„ activation map <br>ç‰¹å¾æ˜ å°„ feature map<br>24 x 24 |
| ReLu activation               |                                          |                                                     |                                                             |
| polling layer                 |                                          |                                                     |                                                             |

[Image convolution examples](http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks)

##### RNN

Recurrent neural network

![neural_network](awsml_pic/rnn.png)

- for Feedforward neural network and convolutional, independent input
- Time series, language, sequencial feature

### **K-Nearest Neighbors**

|  |  |
| ---- | ---- |
| 1, Define a distance measure in the training data<br />2, Apply for new data point<br />3, Comment the observation [é¢„æµ‹é¢„æµ‹ç›®æ ‡å’Œæ‰€æœ‰æ ·æœ¬ä¹‹é—´çš„è·ç¦»æˆ–è€…ç›¸ä¼¼åº¦]<br />4, Identify the nearst neighbors<br />5, Define the k [Small k, local observation, large k, more global]<br />6, vote | ![neural_network](awsml_pic/knn.png)<br />![neural_network](awsml_pic/knn.png) |
| 1, ç®€å•ï¼Œmemory-based, instance based<br />2, é€‚åˆä½çº¬ï¼ˆå°‘featuresï¼‰<br />3, é¢„æµ‹ä¸­è¦å¾ªç¯æ‰€æœ‰æ ·æœ¬ | ![neural_network](awsml_pic/knn3.png) |
|  |  |



more features, more sparse in the space the data point will be

2ï¼Œ é‡è¦ç‰¹å¾çš„å æ¯”è¢«å‰Šå‡ã€ç‰¹å¾é€‰æ‹©ã€‘| è·ç¦»ç®—æ³•çš„å¤æ‚åº¦å’Œå‘é‡çš„é•¿åº¦æˆçº¿æ€§å…³ç³»

4ï¼Œä¸èƒ½å®æ—¶

åªä¿ç•™ä»£è¡¨æ€§æ ·æœ¬ï¼ŒKD-Treeï¼Œè¿‘ä¼¼çš„ç®—æ³• LSH (ç‰ºç‰²ä¸€ç‚¹å‡†ç¡®åº¦)

##### äºŒåˆ†é—®é¢˜

ä¸€èˆ¬å¯¹äºäºŒåˆ†ç±»é—®é¢˜æ¥è¯´ï¼ŒæŠŠKè®¾ç½®ä¸ºå¥‡æ•°æ˜¯å®¹æ˜“é˜²æ­¢å¹³å±€çš„ç°è±¡ã€‚ä½†å¯¹äºå¤šåˆ†ç±»æ¥è¯´ï¼Œè®¾ç½®ä¸ºå¥‡æ•°æœªå¿…ä¸€å®šèƒ½å¤Ÿé˜²å¹³å±€ã€‚ 

```python
from sklearn import datasets
from collections import Counter 
from sklearn.model_selection import train_test_split
import numpy as np

# å¯¼å…¥irisæ•°æ®
iris = datasets.load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2003)

def euc_dis(instance1, instance2):
    """
    è®¡ç®—ä¸¤ä¸ªæ ·æœ¬instance1å’Œinstance2ä¹‹é—´çš„æ¬§å¼è·ç¦»
    instance1: ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œ arrayå‹
    instance2: ç¬¬äºŒä¸ªæ ·æœ¬ï¼Œ arrayå‹
    """
    dist = np.sqrt(sum((instance1 - instance2)**2))
    return dist
    
    
def knn_classify(X, y, testInstance, k):
    """
    ç»™å®šä¸€ä¸ªæµ‹è¯•æ•°æ®testInstance, é€šè¿‡KNNç®—æ³•æ¥é¢„æµ‹å®ƒçš„æ ‡ç­¾ã€‚ 
    X: è®­ç»ƒæ•°æ®çš„ç‰¹å¾
    y: è®­ç»ƒæ•°æ®çš„æ ‡ç­¾
    testInstance: æµ‹è¯•æ•°æ®ï¼Œè¿™é‡Œå‡å®šä¸€ä¸ªæµ‹è¯•æ•°æ® arrayå‹
    k: é€‰æ‹©å¤šå°‘ä¸ªneighbors? 
    """
    # è¿”å›testInstanceçš„é¢„æµ‹æ ‡ç­¾ = {0,1,2}
    distances = [euc_dis(x, testInstance) for x in X]
    kneighbors = np.argsort(distances)[:k]
    count = Counter(y[kneighbors])
    return count.most_common()[0][0]

# å‡†ç¡®ç‡    
predictions = [knn_classify(X_train, y_train, data, 3) for data in X_test]
correct = np.count_nonzero((predictions==y_test)==True)
print ("Accuracy is: %.3f" %(correct/len(X_test)))
```

##### 4 ç‚¹éœ€è¦æ³¨æ„

1, X -- Feature Engineering

2,  éœ€è¦æå‰æ ·æœ¬æ ‡æ³¨

3, è®¡ç®—2ä¸ªç‰©ä½“ä¹‹é—´çš„ç›¸ä¼¼åº¦

- æ¬§æ°è·ç¦» *Euclidean distance*

4, é€‰æ‹©åˆé€‚çš„K

- å†³ç­–è¾¹ç•Œï¼šæ‹¥æœ‰çº¿æ€§å†³ç­–è¾¹ç•Œçš„æ¨¡å‹æˆ‘ä»¬ç§°ä¸ºçº¿æ€§æ¨¡å‹ï¼Œåä¹‹éçº¿æ€§æ¨¡å‹ã€‚
  - çº¿æ€§åˆ†ç±»å™¨
  - éçº¿æ€§åˆ†ç±»å™¨
- æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›

[k è‡ªä¿¡çš„ç¨‹åº¦ï¼Œk = 1 è¿‘çš„äººæ˜¯A æŠ„ä½œä¸š 1ä¸ªå°±å¤Ÿï¼Œ æ—è¾¹éƒ½æ˜¯B éœ€è¦æ›´å¤šçš„K]

collaborative filtering. -- 

##### KNNçš„å†³ç­–è¾¹ç•Œ

-  éšç€Kå€¼çš„å¢åŠ ï¼Œå†³ç­–è¾¹ç•Œç¡®å®ä¼šå˜å¾—æ›´åŠ å¹³æ»‘ï¼Œä»è€Œæ¨¡å‹å˜å¾—æ›´åŠ ç¨³å®šã€‚

- ä½†ç¨³å®šä¸ä»£è¡¨ï¼Œè¿™ä¸ªæ¨¡å‹å°±ä¼šè¶Šå‡†ç¡®ã€‚
![knn_k_1](knn/knn_k_1.png)
![knn_k_1](knn/knn_k_2.png)

##### äº¤å‰éªŒè¯

å°†æ•°æ®åˆ†æˆè®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®ï¼Œé€‰æ‹©åœ¨éªŒè¯æ•°æ®é‡Œæœ€å¥½çš„è¶…å‚æ•°ã€‚

*K-fold Cross Validation* KæŠ˜äº¤å‰éªŒè¯ï¼šå·²æœ‰çš„æ•°æ®ä¸Šé‡å¤åšå¤šæ¬¡çš„éªŒè¯

- é’ˆå¯¹ä¸åŒçš„Kå€¼ï¼Œé€ä¸€å°è¯•ä»è€Œé€‰æ‹©æœ€å¥½çš„
- æ•°æ®é‡è¾ƒå°‘çš„æ—¶å€™æˆ‘ä»¬å–çš„Kå€¼ä¼šæ›´å¤§
- æç«¯æƒ…å†µï¼š*leave_one_out* ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡åªæŠŠä¸€ä¸ªæ ·æœ¬å½“åšéªŒè¯æ•°æ®ï¼Œå‰©ä¸‹çš„å…¶ä»–æ•°æ®éƒ½å½“åšæ˜¯è®­ç»ƒæ ·æœ¬ã€‚

###### è‡ªå·±å†™

```python

from sklean.model_selection import KFold
from sklean.neighbors import KNeighborsClassifier

ks = [1,3,5,7,9,11,13,15]
kf = KFold(n_splits = 5, randon_state = True, shuffle = True)

best_k = ks[0]
best_score = 0

for k in ks:
  curr_score = 0
  for train_idx, valid_idx in kf.split(X):
    clf = KNeighborsClassifier(n_neighbors =k)
    clf.fit(X[train_idx], y[train_idx])
    curr_score = curr_score + clf.score(X[valid_idx],y[valid_idx])
  avg_score = curr_score/5
  if avg_score > best_score:
    best_k = k
    best_score = avg_score
  print("current best score is %.2f"%best_score, "best k: %d"%best_k)
print("after cross validation, the final best k is: %d"%best_k)
  

```
###### GridSearch


```python
from sklean.model_selection import GridSearchCV
from sklean.neighbors import KNeighborsClassifier

parameters = {'n_neighbors':[1,3,5,7,9,11,13,15]} #knn çš„å‚æ•°
knn = KNeighborsClassifier() #ä¸ç”¨æŒ‡å®šå‚æ•°
#GridSearchCVæ¥æœç´¢æœ€å¥½çš„kã€‚æ¨¡å—å†…éƒ¨å¯¹æ¯ä¸ªkå€¼éƒ½åšäº†è¯„ä¼°

clf = GridSearchCV(knn, parameters, cv = 5)
clf.fit(X,y)

print("best score is: %.2f"%clf.best_score_, " best param: ", clf.best_params_)

```

K: 

- å¯¹äºKNNæ¥è®²ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä»K=1å¼€å§‹å°è¯•ï¼Œä½†ä¸ä¼šé€‰æ‹©å¤ªå¤§çš„Kå€¼ã€‚è€Œä¸”è¿™ä¹Ÿå–å†³äºè®¡ç®—ç¡¬ä»¶ï¼Œå› ä¸ºäº¤å‰éªŒè¯æ˜¯ç‰¹åˆ«èŠ±æ—¶é—´çš„è¿‡ç¨‹ï¼Œå› ä¸ºé€ä¸ªéƒ½è¦å»å°è¯•
- æé«˜æ•ˆç‡ï¼šå¹¶è¡ŒåŒ–ã€åˆ†å¸ƒå¼çš„å¤„ç†ã€‚é’ˆå¯¹äºä¸åŒå€¼çš„äº¤å‰éªŒè¯ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå®Œå…¨å¯ä»¥å¹¶è¡ŒåŒ–å¤„ç†ã€‚

**æˆ‘ä»¬ç»å¯¹ä¸èƒ½æŠŠæµ‹è¯•æ•°æ®ç”¨åœ¨äº¤å‰éªŒè¯çš„è¿‡ç¨‹ä¸­**

##### ç‰¹å¾ç¼©æ”¾

- ç‰¹å¾å€¼ä¸Šçš„èŒƒå›´çš„å·®å¼‚å¯¹ç®—æ³•å½±å“éå¸¸å¤§ã€‚
- æ ‡å‡†åŒ–çš„æ“ä½œï¼Œä¹Ÿå°±æ˜¯æŠŠç‰¹å¾æ˜ å°„åˆ°ç±»ä¼¼çš„é‡çº²ç©ºé—´ï¼Œç›®çš„æ˜¯ä¸è®©æŸäº›ç‰¹å¾çš„å½±å“å˜å¾—å¤ªå¤§ã€‚

###### Min-max Normalization

çº¿æ€§å½’ä¸€åŒ–ï¼šæŠŠç‰¹å¾å€¼çš„èŒƒå›´æ˜ å°„åˆ°[0,1]åŒºé—´
<img src= knn/min_max.png  style="zoom:50%" />

###### Z-score Normalization

æ ‡å‡†å·®å½’ä¸€åŒ–ï¼šç‰¹å¾å€¼æ˜ å°„åˆ°å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„æ­£æ€åˆ†å¸ƒ
<img src= knn/zscore.png  style="zoom:50%" />

##### å›¾åƒè¯†åˆ«Knn

http://www.cs.toronto.edu/~kriz/cifar.html

å›¾ç‰‡æ˜¯ç”±åƒç´ æ¥æ„æˆçš„ï¼Œæ¯”å¦‚256*256æˆ–è€…128*128ã€‚ä¸¤ä¸ªå€¼åˆ†åˆ«ä»£è¡¨é•¿å®½ä¸Šçš„åƒç´ ã€‚è¿™ä¸ªå€¼è¶Šå¤§å›¾ç‰‡å°±ä¼šè¶Šæ¸…æ™°ã€‚å¦å¤–ï¼Œå¯¹äºå½©è‰²çš„å›¾ç‰‡ï¼Œä¸€ä¸ªåƒç´ ç‚¹ä¸€èˆ¬ç”±ä¸‰ç»´æ•°ç»„æ¥æ„æˆï¼Œåˆ†åˆ«ä»£è¡¨çš„æ˜¯R,G,Bä¸‰ç§é¢œè‰²ã€‚é™¤äº†RGBï¼Œå…¶å®è¿˜æœ‰å…¶ä»–å¸¸ç”¨çš„è‰²å½©ç©ºé—´ã€‚å¦‚æœä½¿ç”¨RGBæ¥è¡¨ç¤ºæ¯ä¸€ä¸ªåƒç´ ç‚¹ï¼Œä¸€ä¸ªå¤§å°ä¸º128*128åƒç´ çš„å›¾ç‰‡å®é™…å¤§å°ä¸º128*128*3ï¼Œæ˜¯ä¸€ä¸ªä¸‰ç»´å¼ é‡çš„å½¢å¼ã€‚

å›¾åƒçš„è¯»å–åŠè¡¨ç¤ºï¼š

```python
import matplotlib.pyplot as plt

img = plt.imread('.jpg')
print(img.shape)
plt.imshow(img)
```

å›¾åƒè¯†åˆ«æŒ‘æˆ˜ï¼šç¯å¢ƒå› ç´  ï¼ˆæ‹æ‘„è§’åº¦ï¼Œå›¾åƒäº®åº¦ï¼Œé®æŒ¡ç‰©ï¼‰

1ï¼Œå›¾åƒä¸Šçš„ç‰¹å¾å·¥ç¨‹

- é¢œè‰²ç‰¹å¾ï¼ˆcolor histogramï¼‰ 
- SIFTï¼ˆscale-invariant feature transformï¼‰ï¼šä¸€ä¸ªå±€éƒ¨çš„ç‰¹å¾ï¼Œå®ƒä¼šè¯•å›¾å»å¯»æ‰¾å›¾ç‰‡ä¸­çš„æ‹ç‚¹è¿™ç±»çš„å…³é”®ç‚¹ï¼Œç„¶åå†é€šè¿‡ä¸€ç³»åˆ—çš„å¤„ç†æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªSIFTå‘é‡ã€‚
- HOGï¼ˆHistogram of Oriented Gradientï¼‰ï¼šé€šè¿‡è®¡ç®—å’Œç»Ÿè®¡å›¾åƒå±€éƒ¨åŒºåŸŸçš„æ¢¯åº¦æ–¹å‘ç›´æ–¹å›¾æ¥æ„å»ºç‰¹å¾ã€‚ç”±äºHOGæ˜¯åœ¨å›¾åƒçš„å±€éƒ¨æ–¹æ ¼å•å…ƒä¸Šæ“ä½œï¼Œæ‰€ä»¥å®ƒå¯¹å›¾åƒå‡ ä½•çš„å’Œå…‰å­¦çš„å½¢å˜éƒ½èƒ½ä¿æŒå¾ˆå¥½çš„ä¸å˜æ€§ã€‚

2ï¼Œ å›¾åƒé™ç»´

- è¿™ç§é™ç»´æ“ä½œä¼šæ›´å¥½åœ°ä¿ç•™å›¾ç‰‡ä¸­é‡è¦çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¹Ÿå¸®åŠ©è¿‡æ»¤æ‰æ— ç”¨çš„å™ªå£°ã€‚
- PCA ï¼šå¯¹æ•°æ®åšçº¿æ€§çš„å˜æ¢ï¼Œç„¶ååœ¨ç©ºé—´é‡Œé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„Top Kç»´åº¦ä½œä¸ºæ–°çš„ç‰¹å¾å€¼ã€‚

##### ç¼ºå¤±å€¼

1ï¼Œåˆ é™¤ï¼š70%ç¼ºå¤±å€¼ï¼Œåˆ é™¤column

2ï¼Œè¡¥å¹³

##### ç‰¹å¾ç¼–ç feature encoding

###### categorical

- å­—ç¬¦ä¸²è½¬æ¢æˆæ•°å€¼ç±»å‹

Label encoding æ ‡ç­¾ç¼–ç :ä¸€ä¸ªç±»åˆ«è¡¨ç¤ºæˆä¸€ä¸ªæ•°å€¼ï¼Œæ¯”å¦‚0ï¼Œ1ï¼Œ2ï¼Œ3â€¦.

one-hot encoding ç‹¬çƒ­ç¼–ç 
<img src= /Users/yaohanjiang/Desktop/daily/MLæ¨¡å‹/one_hot.png  style="zoom:50%" />

###### Integer æ•°å€¼å‹

æ ‡å‡†åŒ–æ“ä½œ

ç¦»æ•£åŒ–æ“ä½œ Discretization
<img src= knn/discretization.png  style="zoom:50%" />

é˜ˆå€¼çš„ç¡®å®šï¼šä¿è¯å„ä¸ªåŒºé—´çš„æ ·æœ¬ä¸ªæ•°æ˜¯ç±»ä¼¼çš„

1ï¼Œå¢åŠ æ¨¡å‹çš„éçº¿æ€§å‹

2ï¼Œæœ‰æ•ˆå¤„ç†ç†æ•°æ®åˆ†å¸ƒçš„ä¸å‡åŒ€çš„ç‰¹ç‚¹ã€‚

##### Ordinalé¡ºåº

æ¯ä¸€ç§å€¼éƒ½æœ‰å¤§å°çš„å…³ç³»ï¼Œä¹Ÿå°±æ˜¯ç¨‹åº¦ä¸Šçš„å¥½åä¹‹åˆ†



#### äºŒï¼šå›å½’é—®é¢˜
<img src= knn/knnå›å½’ä¸¾ä¾‹.png  style="zoom:50%" />

1ï¼Œç‰¹å¾å¤„ç†

2ï¼Œ Corr() è®¡ç®—ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

3ï¼Œè¿™é‡ŒStandardScalerç”¨æ¥åšç‰¹å¾çš„å½’ä¸€åŒ–ï¼ŒæŠŠåŸå§‹ç‰¹å¾è½¬æ¢æˆå‡å€¼ä¸º0æ–¹å·®ä¸º1çš„é«˜æ–¯åˆ†å¸ƒã€‚

ç‰¹å¾çš„å½’ä¸€åŒ–çš„æ ‡å‡†ä¸€å®šè¦æ¥è‡ªäºè®­ç»ƒæ•°æ®ï¼Œä¹‹åå†æŠŠå®ƒåº”ç”¨åœ¨æµ‹è¯•æ•°æ®ä¸Šã€‚

4ï¼Œä»¥åŠç”¨KNNæ¨¡å‹åšé¢„æµ‹ï¼Œå¹¶æŠŠç»“æœå±•ç¤ºå‡ºæ¥ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†y_normalizer.inverse_transformï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒçš„æ—¶å€™æŠŠé¢„æµ‹å€¼yä¹Ÿå½’ä¸€åŒ–äº†ï¼Œæ‰€ä»¥æœ€åçš„ç»“è®ºé‡ŒæŠŠä¹‹å‰å½’ä¸€åŒ–çš„ç»“æœé‡æ–°æ¢å¤åˆ°åŸå§‹çŠ¶æ€ã€‚ åœ¨ç»“æœå›¾é‡Œï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œå‡å¦‚é¢„æµ‹å€¼å’Œå®é™…å€¼ä¸€æ ·çš„è¯ï¼Œæ‰€æœ‰çš„ç‚¹éƒ½ä¼šè½åœ¨å¯¹è§’çº¿ä¸Šï¼Œä½†å®é™…ä¸Šç°åœ¨æœ‰ä¸€äº›è¯¯å·®ã€‚

<img src= knn/knn_reg_1.png  style="zoom:50%" />
<img src= knn/knn_reg_2.png  style="zoom:50%" />



#### ä¸‰ï¼š å¤æ‚åº¦åˆ†æä»¥åŠKDæ ‘

KNNåœ¨æœç´¢é˜¶æ®µçš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ

- å‡å¦‚æœ‰Nä¸ªæ ·æœ¬ï¼Œè€Œä¸”æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ä¸ºDç»´çš„å‘é‡ã€‚é‚£å¯¹äºä¸€ä¸ªç›®æ ‡æ ·æœ¬çš„é¢„æµ‹ï¼Œéœ€è¦çš„æ—¶é—´å¤æ‚åº¦æ˜¯O(ND)
- å¦‚ä½•æå‡ï¼Ÿ
  - 1ï¼Œæ—¶é—´å¤æ‚åº¦é«˜çš„æ ¹æºåœ¨äºæ ·æœ¬æ•°é‡å¤ªå¤šã€‚æ‰€ä»¥ï¼Œä¸€ç§å¯å–çš„æ–¹æ³•æ˜¯ä»æ¯ä¸€ä¸ªç±»åˆ«é‡Œé€‰å‡ºå…·æœ‰ä»£è¡¨æ€§çš„æ ·æœ¬ã€‚å¦‚ï¼šå¯¹äºæ¯ä¸€ä¸ªç±»çš„æ ·æœ¬åš**èšç±»**ï¼Œä»è€Œé€‰å‡ºå…·æœ‰ä¸€å®šä»£è¡¨æ€§çš„æ ·æœ¬ã€‚
  - 2ï¼Œ å¯ä»¥ä½¿ç”¨è¿‘ä¼¼KNNç®—æ³•ã€‚ç®—æ³•ä»ç„¶æ˜¯KNNï¼Œä½†æ˜¯åœ¨æœç´¢çš„è¿‡ç¨‹ä¼šåšä¸€äº›è¿‘ä¼¼è¿ç®—æ¥æå‡æ•ˆç‡ï¼Œä½†åŒæ—¶ä¹Ÿä¼šç‰ºç‰²ä¸€äº›å‡†ç¡®ç‡ã€‚https://www.cs.umd.edu/~mount/ANN/
  - 3ï¼Œä½¿ç”¨KDæ ‘æ¥åŠ é€Ÿæœç´¢é€Ÿåº¦
    - KDæ ‘çœ‹ä½œæ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Œè€Œä¸”è¿™ç§æ•°æ®ç»“æ„æŠŠæ ·æœ¬æŒ‰ç…§åŒºåŸŸé‡æ–°åšäº†ç»„ç»‡ï¼Œè¿™æ ·çš„å¥½å¤„æ˜¯ä¸€ä¸ªåŒºåŸŸé‡Œçš„æ ·æœ¬äº’ç›¸ç¦»å¾—æ¯”è¾ƒè¿‘ã€‚
    -   <img src= /Users/yaohanjiang/Desktop/daily/MLæ¨¡å‹/kdtree1.png  style="zoom:40%" />
    - KDæ ‘ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨å®ƒæ¥è¾…åŠ©KNNçš„æœç´¢äº†
      -   ä¸ºäº†ä¿è¯èƒ½å¤Ÿæ‰¾åˆ°å…¨å±€æœ€è¿‘çš„ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦é€‚å½“å»æ£€ç´¢å…¶ä»–åŒºåŸŸé‡Œçš„ç‚¹ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¹Ÿå«ä½œBacktrackingã€‚
    - <img src= /Users/yaohanjiang/Desktop/daily/MLæ¨¡å‹/kdtree.png  style="zoom:20%" />
       - æœ€åæƒ…å†µï¼šbacktracking äº†æ‰€æœ‰çš„èŠ‚ç‚¹
         - <img src= /Users/yaohanjiang/Desktop/daily/MLæ¨¡å‹/kdtree2.png  style="zoom:40%" />



#### å››ï¼šå¸¦æƒé‡çš„knn











### **Supervised Learning: Linear and Non-Linear Support Vector Machines**

### **Supervised Learning: Decision Trees and Random Forests**

### Model Training: Validation Set

### **Model Training: Bias Variance Tradeoff**

### **Model Debugging: Error Analysis**

### **Model Tuning: Regularization**

### **Model Tuning: Hyperparameter Tuning**

### **Model Tuning**

### **Model Tuning: Feature Extraction**

### **Model Tuning: Feature Selection**

### **Model Tuning: Bagging/Boosting**





# Math

![math](awsml_pic/math.png)